mode: pipeline
residual: True
data:
  dataset:
    name: ln_coco
    dataset_root: /media/exx/HDD/zhenyulu/data/coco
    clip_image_transform: ViT-L/14
    load_image: true
    load_audio: true
    tokenizeText: true
    modalities: ["audio", "image", "text"]
  batch_size: 32
  dev_batch_size: 32
  split_ratio: 0.9

model_settings:
  modality: ["audio", "image" ] #,'text']
  bifurcated : False
cl_loss:
  type: MaskedContrastiveLoss # SupConLoss
  args:
    temperature: 0.07
    temperature_trainable: False
    margin: 0.0
    dcl: false
    a2b:  true
    b2a: true

    # for SupConLoss
    # temperature: 1.0
    # base_temperature: 1.0
    # contrast_mode: all
    # learnable_temperature: true
retrieval:
  recall_at: [1,5,10]


clip:
  name: ViT-L/14 
  prompt: True
  shared_audio: false
  design_details:
    trainer: 'MaPLe'
    vision_depth: 0
    language_depth: 0 
    vision_ctx: 0
    language_ctx: 0
    maple_length: 4
    compound_prompts_depth: 1

audio_encoder:
  type: FairseqHubert
  name: hubert #hubert hubert_large_ll60k
  pretrained: true
  trainable: false
  feat_select_idx: weighted_sum #last_hidden_state # all
  layer_drop: 0.0
  max_audio_len: 102400
  normalize_hiddenstates: false
  optim:
    name: Adam
    args:
      lr: 1.e-4
      weight_decay: 1.e-6
  scheduler:
    name: linear_warmup_decay
    warmup: 5000
    max_step: 100000
    final_lr: 1.e-8

ASR:
  type: Whisper
  name: base.en

trainer:
  max_steps: 100000
  gradient_clip_val: 4
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  precision: 16
  #logger: wandb
  # log_every_n_steps: 8
  default_root_dir: exp/ln_coco/maple_1_4
  #num_sanity_val_steps: 0
  strategy: dp
  # limit_train_batches: 8
  # limit_val_batches: 8

log_setting:
  log_detokenize_results: true # whether or not to save the results of detokenized vq output
  log_detokenize_results_every_n_epoch: 5
  log_draw_pca_every_n_epoch: 10

logger:
  project: sphclip
