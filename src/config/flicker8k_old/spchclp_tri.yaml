data:
  dataset:
    name: flickr
    dataset_root: /media/exx/HDD/zhenyulu/flickr
    text_file: Flickr8k.token.txt
    clip_image_transform: ViT-B/32
    load_image: true
    load_audio: true
    tokenizeText: true
    modalities: ["audio", "image", "text"]
  batch_size: 32
  dev_batch_size: 4
  split_ratio: 0.9

model_settings:
  modality: ['audio','image','text']
  cascaded_objective_weight: 0.0
  parallel_objective_weight: 1.0
  parallel_branch: 
    # name: integrate # [ "integrate", "individual" ]
    # integrate: add another [CLS] token for parallel objective
    # individual: add another [CLS] token and another branch transformer encoder for parallel objective
    transformer_type: MultiheadAttentionAndNorm #TransformerEncoder
    transformer_args:
        n_layers: 1
        d_model: 768
        nhead: 8
        dim_feedforward: 3072
        dropout: 0.1
        activation: gelu
        layer_norm_eps: 1.0e-5
        batch_first: True
        norm_first: False
    need_projection: true

  cascaded_branch:
    type: KW_CascadedBranch #KW_CascadedBranch #KW_CascadedBranch_Integrated
    transformer_type: MultiheadAttentionAndNorm #[TransformerEncoder, MultiheadAttentionAndNorm, RelTransformerEncoder]
    transformer_args:
        n_layers: 1
        d_model: 768
        nhead: 1
        dim_feedforward: 3072
        dropout: 0.1
        activation: gelu
        layer_norm_eps: 1.0e-5
        batch_first: True
        norm_first: False
    keyword:
        number: 8
        detokenized_K_neighbors: 5
        retrieve_method: cosine # cosine or pseudo_inverse
        batchnorms:
            type: eachKw #eachKw or same
            std_scale: 1.0
            learnable: true
            parallel: true
        attention_constraints:
            diversity_per_kw_loss_weight : 0.0
            diversity_per_frame_loss_weight : 0.0
            smoothness_per_frame_loss_weight : 0.0
    vq:
        bn_before_vq : true
        activation: gelu # relu or gelu
        type: SimpleVectorQuantizer # which type of quantizer to use, gumbel or kmeans
        args:
            temp: fixed=0.1 #[2.0, 0.5, 0.999995]
            time_first: true
            use_gumbel: false
            hard: true
  
cl_loss:
  type: MaskedContrastiveLoss # SupConLoss
  args:
    temperature: 0.07
    temperature_trainable: False
    margin: 0.0
    dcl: false
    a2b:  true
    b2a: true

    # for SupConLoss
    # temperature: 1.0
    # base_temperature: 1.0
    # contrast_mode: all
    # learnable_temperature: true
retrieval:
  audio_feat_src: parallel  # ["parallel","cascaded"]
  recall_at: [1,5,10]


clip:
  name: ViT-B/32
  image_encoder_trainable: false
  text_encoder_trainable: false
  reduce_subword_embbedding: ./avssl/data/flickr_stat/text_clip_vocab_usage_byfreq.npy  # npy file for the selected embedding ID and its frequency


audio_encoder:
  type: FairseqHubert
  name: hubert
  pretrained: true
  trainable: false
  feat_select_idx: weighted_sum #last_hidden_state # all
  layer_drop: 0.0
  max_audio_len: 102400
  normalize_hiddenstates: false
  optim:
    name: Adam
    args:
      lr: 1.e-4
      weight_decay: 1.e-6
  scheduler:
    name: linear_warmup_decay
    warmup: 5000
    max_step: 50000
    final_lr: 1.e-8

trainer:
  max_steps: 50000
  gradient_clip_val: 4
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  precision: 16
  #logger: wandb
  # log_every_n_steps: 8
  # default_root_dir: exp/sphclip_base_p_flickr
  # num_sanity_val_steps: 0
  # strategy: dp
  # limit_train_batches: 8
  # limit_val_batches: 8

log_setting:
  log_detokenize_results: true # whether or not to save the results of detokenized vq output
  log_detokenize_results_every_n_epoch: 5
  log_draw_pca_every_n_epoch: 10

logger:
  project: sphclip
